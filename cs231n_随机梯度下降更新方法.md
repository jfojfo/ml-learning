##普通更新
```python
# 普通更新
x += -learning_rate * dx
```

##动量（Momentum）更新
```python
# 动量更新
v = mu * v - learning_rate * dx # 与速度融合
x += v # 与位置融合
```

##Nesterov动量
```python
x_ahead = x + mu * v
# 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)
v = mu * v - learning_rate * dx_ahead
x += v
```
```python
v_prev = v # 存储备份
v = mu * v - learning_rate * dx # 速度更新保持不变
x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式
```

##Adagrad
```python
# 假设有梯度和参数向量x
cache += dx**2
x += - learning_rate * dx / np.sqrt(cache + eps)
```
the weights that receive high gradients will have their effective learning rate reduced, while weights that receive small or infrequent updates will have their effective learning rate increased

##RMSprop
```python
cache =  decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```

##Adam
```python
m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
```

##效果图
![1](img/opt1.gif)
![1](img/opt2.gif)

##参考：

- [http://cs231n.github.io/neural-networks-3/](http://cs231n.github.io/neural-networks-3/)
- [知乎intelligentunit翻译](https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit)

